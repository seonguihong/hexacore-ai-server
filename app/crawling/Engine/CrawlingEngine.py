import re
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
import asyncio
import requests
from bs4 import BeautifulSoup

from app.crawling.Engine.prompts import STRICT_JSON_PROMPT
from app.post_analysis.infrastructure.service.openai_service_impl import OpenAIServiceImpl


@dataclass
class Article:
    title: str
    content: str
    url: str = ""
    analysis: Optional[Dict[str, Any]] = None


class CrawlingEngine:

    def __init__(self, url: str = ""):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"
        }
        self.OAS = OpenAIServiceImpl()

    def extract_links_from_list_page(self, html: str, page: int) -> List[str]:
        """ëª©ë¡ í˜ì´ì§€ HTMLì—ì„œ ê²Œì‹œê¸€ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        links = []
        soup = BeautifulSoup(html, "lxml")
        articles = soup.find_all("p", attrs={"class": "tit"})

        for article in articles:
            atag = article.find("a", attrs={"class": "best-title"})
            if atag:
                command = atag.get("href")
                match = re.search(r"orgBbsWrtView\((\d+),\s*'([^']+)'\)", command)
                if match:
                    seq = match.group(1)
                    board_id = match.group(2)
                    view_url = f"https://www.paxnet.co.kr/tbbs/view?id={board_id}&seq={seq}&page={page}"
                    links.append(view_url)

        return links

    def parse_article(self, html: str) -> tuple[str, str]:
        """ê²Œì‹œê¸€ HTMLì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html, "lxml")

        # ì œëª©
        title_tag = soup.find("h1")
        title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

        # ë³¸ë¬¸
        content_div = soup.find("div", attrs={"class": "board-view-cont"})
        if content_div:
            content = content_div.get_text(strip=True)
        else:
            content = ""

        return title, content

    def crawl_pages(self, page_count: int = 5) -> List[Article]:
        """ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ê²Œì‹œê¸€ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        all_links = []

        for i in range(1, page_count + 1):
            url = f"https://www.paxnet.co.kr/tbbs/list?tbbsType=L&id=N11023&page={i}"
            res = requests.get(url, headers=self.headers)
            res.raise_for_status()
            links = self.extract_links_from_list_page(res.text, i)
            all_links.extend(links)

        articles = []
        for link in all_links:
            print(f"ğŸ¯ ìŠ¤í¬ë© ì¤‘: {link}")
            res = requests.get(link, headers=self.headers)
            title, content = self.parse_article(res.text)

            # OpenAI ë¶„ì„

            print(f"ì œëª©: {title}")
            print(f"ë³¸ë¬¸: {content[:200]}...")


            articles.append(Article(title=title, content=content, url=link))

        return articles

    # aync test
    async def article_analysis(self, page_count: int = 5) -> List[Article]:
        articles = self.crawl_pages(page_count=page_count)
        return_articles = []
        for article in articles:
            # ì—„ê²©í•œ JSON í˜•ì‹ í”„ë¡¬í”„íŠ¸ë¡œ ê²Œì‹œê¸€ ë¶„ì„ (prompts.pyì—ì„œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ ê°€ëŠ¥)
            analysis = await self.OAS.analyze_stock_post(article.content, prompt_template=STRICT_JSON_PROMPT)

            print(f"ë¶„ì„: {type(analysis)}")
            print(f"ë¶„ì„: {(analysis)}")

            return_articles.append(Article(title=article.title, content=article.content, analysis=analysis))
        return return_articles